{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izsqcV2j3vYD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DU IA en Santé - TP 1\n",
    "---\n",
    "---\n",
    "**Alan Balendran** <br>**François Grolleau**<br>**Raphaël Porcher**<br>\n",
    "\n",
    "CRESS Inserm UMR-1153<br> 01/02/24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This practical session:\n",
    "- Explain the basis of data exploration in python using a dataset from ICU patients\n",
    "- Explain how to train machine learning algorithms and predict on new data\n",
    "\n",
    "The idea is to show the main ideas and not necessarily to go into detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have any questions: \n",
    "- Option 1 (9am-5pm): alan.balendran@u-paris.fr \n",
    "- Option 2 (24/7): https://chat.openai.com/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Versatile programming language\n",
    "- Easy to read and understand\n",
    "- Comes with a vast collection of libraries like *NumPy*, *Pandas*, and *Scikit-learn*. These libraries simplify complex tasks such as data analysis, scientific computation, data visualization, machine learning, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a Jupyter Notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Interactive, open-source web application that allows you to create and share documents containing code, equations, visualizations, and narrative text.\n",
    "\n",
    "- **Code and Markdown Cells**: Jupyter Notebooks consist of cells that can contain either code or formatted text using Markdown. This makes it easy to combine code and explanations in one document.\n",
    "\n",
    "- **Real-time Execution**: Code cells can be executed in real-time, allowing for an interactive and iterative coding experience. Results and visualizations are displayed right beneath the code cell.\n",
    "\n",
    "- **Rich Visualizations**: Jupyter supports inline plotting with libraries like Matplotlib, allowing you to create and visualize graphs and charts directly within the notebook.\n",
    "\n",
    "- **Shareability**: Notebooks can be easily shared and can be exported to various formats, including HTML, PDF, and slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A really quick introduction to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In every programming language, data is stored in variables. One of the advantages of Python is that you don't need to specify the type of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "patient_name = \"Phil Good\" ## String\n",
    "age = 25 ## Integer\n",
    "temperature = 37.5 ## Float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also print the values stored in each variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Printing\n",
    "print(\"Patient Name:\", patient_name)\n",
    "print(\"Age:\", age)\n",
    "print(\"Temperature:\", temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Python offers many ways to print an output, Let's review some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(patient_name, 'is', age, 'and has a body temperature of', temperature, '°C.')\n",
    "\n",
    "print('{} is {} and has a body temperature of {} °C.'.format(patient_name, age, temperature))\n",
    "\n",
    "print('%s is %0.f and has a body temperature of %.1f °C.' % (patient_name, age, temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can perform basic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "height = 170\n",
    "weight = 60\n",
    "bmi = 60/((170/100)**2)\n",
    "print('BMI:', bmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Exercice:**\n",
    "- Calculate the BMI for a patient with the following characteristics:\n",
    "    - Weight: 132.277 lb \n",
    "    - Height: 66.9291 inches\n",
    "    \n",
    "The formula for BMI is given by: \n",
    "$$\\text{BMI} = \\frac{\\text{weight (kg)}}{(\\text{height (m)})^2}= \\frac{\\text{weight (lb)}}{(\\text{height (in)}^2)} \\times 703$$\n",
    "\n",
    "- Print the results with 2 decimals using only the printing parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Lists** are mutable collections where various types of data can be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lists\n",
    "\n",
    "symptoms = [\"fever\", \"cough\", \"fatigue\"]\n",
    "heights = [160, 165, 170, 175]\n",
    "patient_info = [patient_name, age, temperature]\n",
    "\n",
    "print(symptoms)\n",
    "print(heights)\n",
    "print(patient_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Adding Elements to a List\n",
    "symptoms.append(\"headache\")\n",
    "print(\"Updated Symptoms:\", symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Accessing List Elements\n",
    "print(\"First height:\", heights[0])\n",
    "print(\"Last Symptom:\", symptoms[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **dictionary** in Python is a mutable collection that stores **key-value pairs**, allowing for efficient retrieval of values based on their associated keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "patient_info = {\n",
    "    'name': patient_name,\n",
    "    'age': age,\n",
    "    'temperature': temperature\n",
    "}\n",
    "\n",
    "patient_info['height'] = 175\n",
    "\n",
    "print(patient_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dictionaries cannot be accessed using integers. Only the key can return the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "patient_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(patient_info['name'])\n",
    "print(patient_info['age'])\n",
    "print(patient_info['temperature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**For loop** allows to iterate over a collection of items (a list, sequence of integers, dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for symptom in symptoms: ## list\n",
    "    print(\"Patient has\", symptom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,3): ## sequence\n",
    "    print(\"Patient has\", symptoms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for keys, values in patient_info.items(): ## dictionary\n",
    "    print(keys,\":\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Operations using lists are limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "heights = [160, 165, 170, 175]\n",
    "weights = [70, 75, 80, 85]\n",
    "print(heights+weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(heights/weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numpy arrays are a useful objects for peforming numerical operations with lists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating NumPy Arrays\n",
    "heights = np.array([160, 165, 170, 175])\n",
    "weights = np.array([70, 75, 80, 85])\n",
    "\n",
    "bmi = weights / ((heights / 100) ** 2)\n",
    "print(\"BMI:\", bmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Python, a **function** is a block of reusable code that performs a specific task. It encapsulates a set of instructions that can be called and executed multiple times. A function can take arguments that will be used inside the function. Functions help in organizing code, promoting reusability, and improving readability. <br>\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_info(patient_dict, print_name=False):\n",
    "    \"\"\"\n",
    "    Prints information from a dictionary representing patient data.\n",
    "\n",
    "    Args:\n",
    "    - patient_dict (dict): A dictionary containing patient information.\n",
    "    - print_name (bool, optional): If True, prints the patient's name. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for key, value in patient_dict.items():\n",
    "        # Check if the key is 'name' and print_name is False, then skip printing it\n",
    "        if key == \"name\" and not print_name:\n",
    "            pass\n",
    "        else:\n",
    "            # Print the key-value pair\n",
    "            print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print_info(patient_info) ## with default value for print_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_info(patient_info, print_name=True) ## with print_name value set to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now move on to our real dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's make sure to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now import common python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np ## for numerical operations\n",
    "import pandas as pd ## for data analysis and manipulation\n",
    "import matplotlib.pyplot as plt ## for visualization (basic)\n",
    "import seaborn as sns ## for visualization (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start by loading our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hm7_YGgm3vYW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('icu_training.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This dataset is a synthetic subset of the [MIMIC-III](https://mimic.mit.edu/) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A good reflex to have is to print the first lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that the data has been read correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3EnqeqW3vYY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can print its dimensions using the `.shape` attribute of a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeMAJeV53vYZ",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our dataset consists of 5000 rows and 32 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbq2Z6S73vYa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's have a look at the different columns of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQcpwN_h3vYb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Outcome variables (what we want to predict)**<br>\n",
    "    - **hospital_mortality**: Hospital mortality with 1 corresponding to in-hospital death and 0 for discharge alive<br>\n",
    "    - **los_icu**: Length of stay in ICU (in days) \n",
    "    \n",
    "    \n",
    "- **Covariates** (features that can be used to predict the outcome)<br>\n",
    "- *Variables used in the [SAPS II score](https://www.mdcalc.com/calc/4044/simplified-acute-physiology-score-saps-ii#use-cases) assessed by clinicians on admission. High scores correspond to greater severity for the patient.*\n",
    "\n",
    "    - **age_score**: score derived from age <br>\n",
    "    - **hr_score**: score derived from heart rate<br>\n",
    "    - **sysbp_score**: score derived from systolic blood pressure<br>\n",
    "    - **temp_score**: score derived from temperature<br>\n",
    "    - **pao2fio2_score**: scored derived from PaO2/FiO2 Fraction of inspired oxygen<br>\n",
    "    - **uo_score**: scored derived from urine output<br>\n",
    "    - **bun_score**: score derived from urea nitrogen present in the blood<br>\n",
    "    - **wbc_score**: score derived from leucocytes (white blood cells) values<br>\n",
    "    - **potassium_score**: score derived from potassium values<br>\n",
    "    - **sodium_score**: score derived from sodium values<br>\n",
    "    - **bicarbonate_score**: score derived from bicarbonate values<br>\n",
    "    - **bilirubin_score**: score derived from bilirubin values<br>\n",
    "    - **gcs_score**: Glasgow score<br>\n",
    "    - **comorbidity_score**: score derived from chronic disease<br>\n",
    "    - **admissiontype_score**: admission type<br>    \n",
    "    - **sapsii**: score from 0 to 110, high values indicates a greater severity.<br>\n",
    "    - **sapsii_prob**: probability of in-hospital mortality derived from the score<br> <br>\n",
    "\n",
    "- *Demographic and laboratory tests*\n",
    "    - **subject_id** patient id\n",
    "    - **age** <br>\n",
    "    - **gender** (F/M) <br>\n",
    "    - **weight** (kg) <br>\n",
    "    - **height** (cm) <br>\n",
    "    - **first_icu_stay**: first stay in icu <br>\n",
    "    - **CURR_CAREUNIT_transfers**: current care unit: MICU for medical intensive care unit and SICU for surgical intensive care unit <br>\n",
    "    - **bun_min**: maximum urea (mM) in the first 24 hours <br>\n",
    "    - **hemoglobin_max**: maximum hemoglobin (g/dl) in the first 24 hours <br>\n",
    "    - **lactate_max**: maximum lactate (mM) in the first 24 hours <br>\n",
    "    - **creatinine_max**: maximum creatinine (mg/dl) in the first 24 hours <br>\n",
    "    - **ptt_max**: maximum partial thromboplastin time in the first 24 hours <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can access a specific column in a DataFrame by using square brackets and specifying the column name, like this: `df['column name']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['lactate_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df[['lactate_max', 'hospital_mortality']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Exercise**:\n",
    "\n",
    "- Find and print the corresponding columns for each of the following variables in the DataFrame:\n",
    "    - *Heart rate score*\n",
    "    - *Age*\n",
    "    - *Gender*\n",
    "    - *First stay in the ICU*\n",
    "\n",
    "- Describe the characteristics of each variable based on their column values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Basic statistics, such as the *mean, minimum*, and *maximum*, can be calculated in pandas using the appropriate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['age'].min()\n",
    "print(\"The youngest patient in our dataset is {:.1f} years old.\".format(df['age'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The oldest patient in our dataset is {:.0f} years old.\".format(df['age'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Exercise**:\n",
    "\n",
    "- Find the highest and lowest values of the variable *hemoglobin_max* in the dataset.\n",
    "- Calculate the average height in the dataset.\n",
    "- Calculate the median lactate value in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also create a subset of your dataset by applying filters based on specific conditions.\n",
    "- Using standard comperators `<`, `<=`, `>`, `>=`, `==`, `!=`.\n",
    "- Combine two conditions with **AND** using the `&` operator.\n",
    "- Combine two conditions with **OR** using the `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Selecting only patient' first visit in ICU:\n",
    "df[df[\"first_icu_stay\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Selecting patients between 20 and 40:\n",
    "df[(df['age']>=20) & (df['age']<=40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df[((df['age']>=20) & (df['age']<=40))| df['first_icu_stay']==True]['height'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The code above returns the maximum height among patients who are between 20 and 40 or patients who have been to the ICU for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Exercise**:\n",
    "\n",
    "- How many patients have a lactate above 1.9 mM or a creatine value above 1.2 mg/dL? How many of those patients died in the hospital?\n",
    "- Find the longest ICU stay among patients aged between 40 and 80.\n",
    "- Calculate the average height for both men and women.\n",
    "- Identify the patient with the highest number of visits. You can use the `value_counts()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can compute basic statistics on the whole dataset directly using the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data visualization\n",
    "\n",
    "\n",
    "While descriptive statistics offer insights into the data, they must be interpreted cautiously. Data visualization, however, can reveal valuable insights that descriptive statistics may overlook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Categorical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='hospital_mortality');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Continuous data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='hemoglobin_max');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Categorical/Categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='hospital_mortality', hue='gender');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Continuous/Continuous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='creatinine_max', y='bun_min', alpha=.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Categorical/Continuous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='hospital_mortality', y='age');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Adding a third information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='bun_min', y='lactate_max', hue='hospital_mortality', alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Exercise**:\n",
    "- Plot *gender* and *age* in the same figure.\n",
    "- Visualize the *temp_score* feature. What observations can you make?\n",
    "- Create a plot of *age_score* against *age* and interpret the scoring rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many other ways to represents data, depending on the type of the data and the information you want to represent. <br>The [Data to viz](https://www.data-to-viz.com) website lists many visualization methods along with the corresponding python (and R) code for replication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "\n",
    "Data pre-processing plays a crucial role prior to model development. It encompasses several essential steps, including handling missing values through imputation or removal, addressing outliers, data encoding, feature engineering, and data standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To start, we will keep only a subset of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "keep = {\n",
    "    \"age\", \"gender\", \"height\", \"weight\", \"CURR_CAREUNIT_transfers\" , \n",
    "    \"hr_score\", \"sysbp_score\", \"pao2fio2_score\",\n",
    "    \"bun_min\", \"hemoglobin_max\", \"lactate_max\", \"creatinine_max\",\n",
    "    \"ptt_max\", \"first_icu_stay\", \"hospital_mortality\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = df[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Among these features, some, such as *gender* and *first_icu_stay*, are non-numeric values. Most machine learning algorithms can only process data in numerical form.\n",
    "\n",
    "Therefore, we need a method to encode these variables. This can be done in two ways:\n",
    "\n",
    "- **Manual Encoding**: This involves defining the appropriate mapping or transformation for each categorical variable. You can manually assign numerical values or create your custom encoding schemes tailored to your specific requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbvXzO_W3vYd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df['gender'].unique()) ## print unique values of that features\n",
    "df['gender'] = df['gender'].replace({'M': 1, 'F': 0}) ## encode the values with predefined mapping\n",
    "print(df['gender'].unique()) ## print uniques values of encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df['first_icu_stay'].unique())\n",
    "df['first_icu_stay'] = df['first_icu_stay'].replace({True: 1, False: 0})\n",
    "print(df['first_icu_stay'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Automatical encoding with scikit-learn package**: The scikit-learn package provides a convenient solution for categorical variable encoding through its `LabelEncoder` function. This function automatically converts categorical values into numeric labels, simplifying the encoding process.\n",
    "\n",
    "The **scikit-learn** package is a python library that provides a set of python modules for machine learning and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder() ## instantiate the LabelEncoder object\n",
    "\n",
    "encoder.fit(df['CURR_CAREUNIT_transfers']) ## \"Train\" the encoder to form a mapping\n",
    "print(encoder.classes_) ## print the unique values that the encoder has identified\n",
    "\n",
    "df['CURR_CAREUNIT_transfers'] = encoder.transform(df['CURR_CAREUNIT_transfers']) ## encode the variable  \n",
    "\n",
    "print(df['CURR_CAREUNIT_transfers'].unique()) ## print uniques values of encoded features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66M86PL53vYo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature correlation \n",
    "\n",
    "Pairwise correlation provides information about which features are correlated. The method `.corr()` method computes the correlation matrix of our dataset. This can help to remove redundant features (correlation of ±1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "corr_matrix = df.corr(method='pearson')\n",
    "mask = np.zeros_like(corr_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "sns.heatmap(corr_matrix, mask=mask,annot=True, cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Exercise**:\n",
    "\n",
    "- Using the provided heatmap, determine the correlation between *creatinine_max* and *bun_min*. Plot a scatterplot to visualize this correlation.\n",
    "- Calculate and create the *BMI (Body Mass Index)* feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization is a common practice in data preprocessing. It involves scaling the data to have a **mean of 0** and a **standard deviation of 1**. This process is performed to prevent features with different scales from disproportionately influencing the training process. Additionally, standardization can improve the performance of certain models during training. *Standardization* is not the only way to scale the data, for example *normalization* consists in rescaling the values between [0,1] or [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('hospital_mortality', axis=1)\n",
    "y = df['hospital_mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_mean = X.mean()\n",
    "train_std = X.std()\n",
    "X = (X-train_mean)/train_std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can check if the data has been correctly standardized by checking the new mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmlkAyvJ3vYp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling <a id=\"part2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz64coQh3vYq",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 1. LASSO (Logistic Regression with L1 penalty)\n",
    "Lasso is a variant of the Logistic Regression that introduces an L1 penalty to the cost function, promoting the shrinkage of coefficients to zero and reducing model complexity.\n",
    "#### 2. Decision Tree\n",
    "Decision Tree is a versatile algorithm that partitions the data based on features at each node, making sequential decisions to predict the target variable. It provides interpretable rules for decision-making.\n",
    "#### 3. Random Forest\n",
    "Random Forest is an ensemble algorithm that combines multiple Decision Trees. It aggregates predictions from individual trees to improve accuracy, providing robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzXbv4aG3vYs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import confusion_matrix, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression() ## instantiate the model\n",
    "clf.fit(X,y) ## train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predictions on new data can be obtained in two ways for a classifier:\n",
    "\n",
    "- Obtain the probability (or score) of each label using the `predict_proba()` method, which returns the estimated probabilities for each class. This is useful when we want to assess the likelihood of each label being the correct one.\n",
    "- Predict the label directly using the `predict()` method, which returns the predicted class label based on the highest probability (or score) among all the classes. By default, sklearn models will use a threshold of 0.5 to get the labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(X.head(1))\n",
    "print('Predicted probabilities', clf.predict_proba(X.head(1)))\n",
    "clf.predict(X.head(1))\n",
    "print('Predicted target', clf.predict(X.head(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will utilize synthetic data to demonstrate the specificities of the different algorithms. The synthetic dataset created using one of the function provided by scikit-learn consists in two classes, represented by the blue and orange points. Our objective is to identify the most effective classifier capable of accurately distinguishing between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_dataset = make_circles(n_samples=500, noise=0.3, factor=0.2, random_state=32),\n",
    "\n",
    "X_synth = synthetic_dataset[0][0]\n",
    "y_synth = synthetic_dataset[0][1]\n",
    "\n",
    "X0, X1 = X_synth[:, 0], X_synth[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create a meshgrid for visualization\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "# Function to plot decision boundaries\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "# Function to plot all components (data points and decision boundaries)\n",
    "def plot_all(model, X=X_synth, y=y_synth, xx=xx, yy=yy, figsize=(6,6), ax=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    plot_contours(ax, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    sns.scatterplot(x=X[:,0],y=X[:,1], hue=y, ax=ax, palette='deep')\n",
    "    \n",
    "# Function to plot the classification summary    \n",
    "def plot_classification_summary(clf, X, y, figsize=(24,6)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3, figsize=figsize);\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    sns.scatterplot(x=X[:,0],y=X[:,1], hue=y, ax=ax[0], palette='deep')\n",
    "\n",
    "    plot_all(clf, X=X, y=y, ax=ax[1])\n",
    "    ax[1].set_title('Data')\n",
    "\n",
    "    plot_cm(clf, X, y, ax=ax[2])\n",
    "    ax[2].set_title('Confusion matrix')\n",
    "    \n",
    "# Function to plot the confusion matrix    \n",
    "def plot_cm(clf, X, y, figsize=(7,7), ax=None):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y, clf.predict(X))\n",
    "    group_names = [\"TN\",\"FP\",\"FN\",\"TP\"]\n",
    "    group_counts = conf_matrix.flatten()\n",
    "    labels = [str(v1)+\"\\n\\n\"+str(v2) for v1, v2 in zip(group_names,group_counts)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='GnBu', cbar=False); \n",
    "        plt.ylabel('True label');\n",
    "        plt.xlabel('Predicted label');\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='GnBu', cbar=False, ax=ax); \n",
    "        ax.set_ylabel('True label');\n",
    "        ax.set_xlabel('Predicted label');\n",
    "\n",
    "# Function to plot ROC curves        \n",
    "def plot_roc_curves(models, X, y, figsize=(12,8)):\n",
    "\n",
    "    fig,ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    if isinstance(models, dict):\n",
    "        for name,model in models.items():\n",
    "            RocCurveDisplay.from_estimator(model, X, y, ax=ax,name=name)\n",
    "    else:\n",
    "        RocCurveDisplay.from_estimator(models, X, y, ax=ax)\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier (AUC = 0.5)\")\n",
    "    plt.title('ROC curves')\n",
    "    plt.legend();\n",
    "\n",
    "# Function to plot ROC curves for training and testing data\n",
    "def plot_roc_curves_sbs(models, X_train, y_train, X_test, y_test, figsize=(24,9)):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=figsize);\n",
    "    ax = ax.flatten()\n",
    "    if isinstance(models, dict):\n",
    "        for name,m in models.items():\n",
    "            RocCurveDisplay.from_estimator(m, X_train, y_train, ax=ax[0])\n",
    "            RocCurveDisplay.from_estimator(m, X_test, y_test, ax=ax[1])\n",
    "    else:\n",
    "        RocCurveDisplay.from_estimator(models, X_train, y_train, ax=ax[0])\n",
    "        RocCurveDisplay.from_estimator(models, X_test, y_test, ax=ax[1])        \n",
    "    ax[0].plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier (AUC = 0.5)\")\n",
    "    ax[1].plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier (AUC = 0.5)\")\n",
    "    ax[0].set_title('Training set')\n",
    "    ax[1].set_title('Testing set')\n",
    "    \n",
    "    \n",
    "# Function to plot calibration curves            \n",
    "def plot_calibration_curve(models, X, y, figsize = (12, 8)):\n",
    "# the following code is taken from https://scikit-learn.org/0.24/auto_examples/calibration/plot_calibration_curve.html\n",
    "# the code will plot the calibration curve for models available in a dictionary\n",
    "    \n",
    "    fig,ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    for name, model in models.items():\n",
    "\n",
    "        prob_pos = model.predict_proba(X)[:, 1]\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y, prob_pos, n_bins=10)\n",
    "        ax.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (name))\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_title('Calibration curve')\n",
    "    plt.tight_layout()    \n",
    "    \n",
    "# Function to plot calibration curves for training and testing data\n",
    "def plot_calibration_curves_sbs(models, X_train, y_train, X_test, y_test, figsize = (24, 9)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=figsize);\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        \n",
    "        prob_pos = model.predict_proba(X_train)[:, 1]\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y_train, prob_pos, n_bins=10)\n",
    "        ax[0].plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (name))\n",
    "    \n",
    "        prob_pos_test = model.predict_proba(X_test)[:, 1]\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos_test, n_bins=10)\n",
    "        ax[1].plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (name))\n",
    "\n",
    "    ax[0].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    ax[1].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    \n",
    "    ax[0].set_ylabel(\"Fraction of positives\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].set_title('Calibration curve - Training data')\n",
    "    \n",
    "    ax[1].set_ylabel(\"Fraction of positives\")\n",
    "    ax[1].legend(loc=\"lower right\")\n",
    "    ax[1].set_title('Calibration curve - Testing data')    \n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,6));\n",
    "sns.scatterplot(x=X_synth[:,0],y=X_synth[:,1], hue=y_synth, palette='deep');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO (Logistic regression with L1 penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf_logistic = LogisticRegression(penalty='l1', solver='saga')\n",
    "clf_logistic.fit(X_synth,y_synth);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot_all(model=clf_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,8))\n",
    "tree = DecisionTreeClassifier(max_depth=1).fit(X_synth,y_synth)\n",
    "plot_all(model=tree, ax= ax[0])\n",
    "ax[0].set_title('Max depth = {}'.format(1))\n",
    "plot_tree(tree, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,8))\n",
    "tree = DecisionTreeClassifier(max_depth=2).fit(X_synth,y_synth)\n",
    "plot_all(model=tree, ax= ax[0])\n",
    "ax[0].set_title('Max depth = {}'.format(2))\n",
    "plot_tree(tree, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,8))\n",
    "tree = DecisionTreeClassifier(max_depth=4).fit(X_synth,y_synth)\n",
    "plot_all(model=tree, ax= ax[0])\n",
    "ax[0].set_title('Max depth = {}'.format(4))\n",
    "plot_tree(tree, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_synth,y_synth);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot_all(model=clf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model selection\n",
    "\n",
    "Model selection involves determining the aspects of a model that can be optimized and selecting the optimal values for the corresponding parameters. It is essential to understand both:\n",
    "- the tunable aspects of a model.\n",
    "- the techniques for choosing the most suitable parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.Model parameters\n",
    "Each model has a certain level of complexity that can be adjusted by tuning its *parameters*.\n",
    "\n",
    "- **[Logistic regression (or LASSO)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** :\n",
    "    - Parameter `C` that controls the penalization value imposed on the coefficients. There are two main ways to penalize the coefficients: *L1 norm (LASSO)* or *L2 norm (RIDGE)*. In scikit learn a smaller value of `C` indicates higher penalization.\n",
    "- **[Decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)**:\n",
    "    - Maximum depth a tree can reach: `max_depth`\n",
    "     \n",
    "- **[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**:\n",
    "    - Number of trees in the ensemble: `n_estimators`\n",
    "    - Number of features selected to construct a tree: `max_features`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Model Evaluation Metrics\n",
    "\n",
    "To quantify the predictive performance of a model, it is essential to utilize appropriate evaluation metrics. Several metrics have been developed, tailored to different types of outcomes of interest. For more information on different evaluation metrics, please refer to the dedicated [scikit-learn webpage](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For binary classification, we have metrics such as *accuracy*, *specificity*, *sensitivity*, those different metrics can be derived from the **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_synth, y_synth)\n",
    "cf_matrix = confusion_matrix(y_synth, clf.predict(X_synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_cm(clf, X_synth, y_synth, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Accuracy:    $\\frac{TP+TN}{TP+TN+FN+FP}$\n",
    "\n",
    "#### Sensitivity:   $\\frac{TP}{TP+FN}$\n",
    "\n",
    "#### Specificity:   $\\frac{TN}{TN+FP}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also directly compute the accuracy using functions provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_synth) ## get the predictions\n",
    "accuracy_score(y_true=y_synth, y_pred=y_pred) ## compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_dataset = make_circles(n_samples=(450,25), noise=0.3, factor=0.2, random_state=32),\n",
    "\n",
    "X_synth_imb = synthetic_dataset[0][0]\n",
    "y_synth_imb = synthetic_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, it is important to exercise caution when relying solely on one metric, such as *accuracy*, as they can sometimes be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_synth_imb, y_synth_imb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_classification_summary(dummy_clf, X_synth_imb, y_synth_imb, figsize=(25,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_synth_imb, y_pred=dummy_clf.predict(X_synth_imb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While accuracy is a straightforward metric to interpret, it presents some limitations:\n",
    "\n",
    "- it is not well-suited for evaluating models on imbalanced datasets, which are commonly encountered in medical data. When the classes are imbalanced, a model that always predicts the majority class can achieve a high accuracy but may not perform well in identifying the minority class.\n",
    "- it solely relies on the hard predictions of the model obtained by fixing a threshold (generally 0.5) and does not consider the probabilities assigned to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In such scenarios, alternative evaluation metrics, that provide a more comprehensive understanding of the model' performance should be favored.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "perfect_clf = RandomForestClassifier()\n",
    "perfect_clf.fit(X, y)\n",
    "good_clf = RandomForestClassifier(n_estimators=100, max_depth=8)\n",
    "good_clf.fit(X, y)\n",
    "average_clf = LogisticRegression()\n",
    "average_clf.fit(X, y)\n",
    "\n",
    "roc_models = {\n",
    "    \"Perfect classifier\": perfect_clf,\n",
    "    \"Good classifier\": good_clf,\n",
    "    \"Average classifier\": average_clf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **Receiver Operating Characteristic (ROC)** curve is obtained by plotting the *true positive rate (sensitivity)* against the *false positive rate (1-specificity)* at various thresholds. The ROC curve represents the ability of a model to discriminate between the two class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_curves(roc_models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The associated metric, the **ROC - Area Under the Curve (AUC)**, is a value that represents the discrimination of the model across all possible thresholds. It ranges from 0 to 1, with a higher value indicating better discrimination ability. \n",
    "\n",
    "The **ROC-AUC** can also be interpreted as the probability that, for two randomly chosen pairs of positive and negative observations, the model's score for the positive sample is higher than that for the negative sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Calculating the ROC-AUC is straightforward using the `roc_auc_score` function. To compute it, you need to provide the true labels and predicted scores (such as predicted probabilities) as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_proba = clf_rf.predict_proba(X_synth)[:,1] ## here we use the predicted probabilities \n",
    "roc_auc_score(y_true=y_synth, y_score=y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we understand which parameters to tune and how to evaluate the performance of a model, we can now perform parameter tuning and select the optimal parameter values for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Define a grid of values  \n",
    "C_values = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "## Define a dict to keep track of the different performance values\n",
    "lasso_score = {}\n",
    "## loop over each potential value and compute the corresponding performance\n",
    "for k in C_values:\n",
    "    ## instantiate a model with a specific value\n",
    "    clf = LogisticRegression(penalty='l1', C=k, solver='saga', max_iter=500) \n",
    "    ## train the model\n",
    "    clf.fit(X,y) \n",
    "    ## compute the performance\n",
    "    lasso_score[\"C = \" +str(k)] = roc_auc_score(y, clf.predict_proba(X)[:,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(lasso_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Exercice**\n",
    "- Similar to the lasso algorithm, tune the different parameters for the decision tree and random forest algorithm using the roc-auc.\n",
    "- Compare with the parameters obtained when using the accuracy instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "After finding the optimal value for each model parameter, we can train the model one final time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LASSO': LogisticRegression(penalty='l1', C=0.1, solver=\"saga\", max_iter=500),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=None),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50)    \n",
    "}\n",
    "for m in models.values():\n",
    "    m.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_curves(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another thing we can consider is the *calibration* of the model. \n",
    "<br>*Calibration* indicates if the predicted probabilities of a model actually align with the actual probabilities or outcomes observed in the data. In other words, a calibrated model accurately estimates the likelihood or confidence of a certain event occurring.<br>\n",
    "\n",
    "For example, when a model is well-calibrated, it means that if the model predicts a probability of 0.8 for an event, the event is expected to occur approximately 80% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_calibration_curve(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation\n",
    "\n",
    "Okay, so our models are performing well on the training data. However, it is important to assess their performance on new, unseen data. Evaluating the models on new data helps us assess if they can make accurate predictions beyond the data they were trained on. This step is crucial to ensure that the models have learnt relevant pattern in the data and can perform effectively on new patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's load a new set of observations and see how well our different models do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('icu_testing.csv', index_col=0)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have to make sure that the new data are in the same format and scale as the one used for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Exercice**\n",
    "- Similar to to training data, apply the same preprocessing steps to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_test['gender'] = \n",
    "df_test['first_icu_stay'] =\n",
    "df_test['CURR_CAREUNIT_transfers'] = \n",
    "df_test['BMI'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test = \n",
    "y_test = \n",
    "X_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Assignment**\n",
    "\n",
    "- Explore the notebook by running the different cells, doing the exercices, and familiarize yourself with the various topics covered today.\n",
    "- Evaluate your models on the testing data and comment for each model how the performances change between the training and testing data.\n",
    "- Reflect on your findings and consider the implications for the models' generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_curves_sbs(models, X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_calibration_curves_sbs(models, X, y, X_test, y_test, figsize = (24, 9))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "colab": {
   "name": "TP_DU_IA_SANTE_jupyter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
