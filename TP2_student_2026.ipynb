{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TP 2\n",
    "---\n",
    "---\n",
    "**Alan Balendran** <br> **Celine Beji** <br> **Etienne Peyrot** <br>**François Grolleau**<br>**Raphaël Porcher**<br> \n",
    "\n",
    "\n",
    "[Centre de Recherche en Epidémiologie et Statistiques (CRESS) - Equipe METHODS](https://cress-umr1153.fr/fr/teams/methods/)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goals of this practical are to:\n",
    "- Understand the notion of overfitting.\n",
    "- Learn how to correctly train and estimate the performance of a prediction model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will first import libraries, some of which we used during the previous practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## basic libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "## preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "## modeling\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## evaluation\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, \\\n",
    "    confusion_matrix, RocCurveDisplay\n",
    "from sklearn.calibration import calibration_curve, CalibrationDisplay\n",
    "\n",
    "## hide warning message\n",
    "pd.set_option('future.no_silent_downcasting', True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the last practical, we learned how to train various machine learning models and make predictions. <br>Let's recap the different steps (We will exclude the section on visualization covered during the first practical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, we start by loading our training dataset `icu_training.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/AL0UNE/courses/refs/heads/main/icu_training.csv'\n",
    "\n",
    "df = pd.read_csv(url_data, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We select a subset of variables to train our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "keep = [\n",
    "    \"age\", \"gender\", \"height\", \"weight\", \"CURR_CAREUNIT_transfers\" , \n",
    "    \"hr_score\", \"sysbp_score\", \"pao2fio2_score\",\n",
    "    \"bun_min\", \"hemoglobin_max\", \"lactate_max\", \"creatinine_max\",\n",
    "    \"ptt_max\", \"first_icu_stay\", \"hospital_mortality\"\n",
    "]\n",
    "\n",
    "df = df[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then apply the different preprocessing steps before training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We impute missing values, in our case, for `height` and `first_icu_stay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_height = df['height'].mean()\n",
    "most_frequent_first_icu_stay = df['first_icu_stay'].mode()[0]\n",
    "\n",
    "df = df.fillna({\"height\": mean_height, \"first_icu_stay\": most_frequent_first_icu_stay})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We encode categorical features, which can either be done manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# we (manually) encode categorical features\n",
    "df['gender'] = df['gender'].replace({'M': 1, 'F': 0}) \n",
    "df['first_icu_stay'] = df['first_icu_stay'].replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['CURR_CAREUNIT_transfers'])\n",
    "df['CURR_CAREUNIT_transfers'] = encoder.transform(df['CURR_CAREUNIT_transfers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When possible and relevant, we can engineer or create new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['BMI'] = df['weight']/((df['height']/100)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then define the covariates $X$ (patient characteristics) and the target $y$ (hospital mortality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('hospital_mortality', axis=1)\n",
    "y = df['hospital_mortality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we standardize our data $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Data standardization\n",
    "\n",
    "train_mean = X.mean()\n",
    "train_std = X.std()\n",
    "X = (X-train_mean)/train_std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We learned that to train a machine learning model, we first need to instantiate the model we want to train (in this case a logistic regression). Then, we train it on our training data (here $X$ and $y$) using the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression() \n",
    "clf.fit(X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once trained, the model can be used to make prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predictions can be obtained in two ways:\n",
    "- probabilities using the `predict_proba()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict_proba(X.head(1))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- binary target using the `predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(X.head(1))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cm(clf, X, y, normalized=None, figsize=(7, 7), ax=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with annotations.\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(y, clf.predict(X), normalize=normalized)\n",
    "    group_names = [\"True Negative (TN)\", \"False Positive (FP)\", \"False Negative (FN)\", \"True Positive (TP)\"]\n",
    "    group_counts = [f\"{value}\" for value in conf_matrix.flatten()]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names, group_counts)]\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(conf_matrix, annot=labels, fmt=\"\", linecolor='lightblue',linewidths=2, cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_ylabel('True label', fontsize=15)\n",
    "    ax.set_xlabel('Predicted label', fontsize=15)\n",
    "    ax.set_title(f\"Confusion Matrix for {type(clf).__name__}\", fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "def plot_curves(models, X_train, y_train, X_test=None, y_test=None, curve_type='discrimination', figsize=(8, 8)):\n",
    "    \"\"\"\n",
    "    Plot ROC or calibration curves for training and testing data.\n",
    "    The figsize is applied per subplot.\n",
    "    \"\"\"\n",
    "    is_test_data_present = X_test is not None and y_test is not None\n",
    "    num_plots = 2 if is_test_data_present else 1\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(figsize[0] * num_plots, figsize[1]))\n",
    "    \n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "\n",
    "    data_splits = [(X_train, y_train)]\n",
    "    if is_test_data_present:\n",
    "        data_splits.append((X_test, y_test))\n",
    "\n",
    "    for i, (X_data, y_data) in enumerate(data_splits):\n",
    "        ax = axes[i]\n",
    "        if curve_type == 'discrimination':\n",
    "            for j, (name, model) in enumerate(models.items()):\n",
    "                plot_chance_level = j == len(models) - 1  # Plot chance level only for the last model\n",
    "                RocCurveDisplay.from_estimator(model, X_data, y_data, ax=ax, name=name, linewidth=3, plot_chance_level=plot_chance_level)\n",
    "        elif curve_type == 'calibration':\n",
    "            if isinstance(models, dict):\n",
    "                for name, model in models.items():\n",
    "                    CalibrationDisplay.from_estimator(model, X_data, y_data, ax=ax, name=name)\n",
    "            else: # handles single model case\n",
    "                CalibrationDisplay.from_estimator(models, X_data, y_data, ax=ax)\n",
    "\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        type_of_plot = \"ROC\" if curve_type == 'discrimination' else \"Calibration\"\n",
    "        data_label = \"Training\" if i == 0 else \"Testing\"\n",
    "        ax.set_title(f'{type_of_plot} curve - {data_label} data', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a set of predictions and known outcome, we can evaluate the predictive ability of our model using various metrics. <br>For instance, we can calculate the confusion matrix for the logistic regression model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_cm(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Accuracy**: fractions of correctly classified observations.    $\\frac{TP+TN}{TP+TN+FN+FP}$ to estimate $\\mathbb P(\\hat Y = Y)$\n",
    "\n",
    "**Sensitivity**: proportion of positive observations correctly classified.   $\\frac{TP}{TP+FN}$ to estimate $\\mathbb P(\\hat Y = 1 \\mid Y = 1)$\n",
    "\n",
    "**Specificity**: proportion of negative observations correctly classified.   $\\frac{TN}{TN+FP}$ to estimate $\\mathbb P(\\hat Y = 0 \\mid Y = 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, in some cases, accuracy may not be the most suitable metric, as it **depends on a specific classification threshold (e.g., 0.5)**. The choice of this threshold often involves a trade-off between sensitivity and specificity, **depending on the clinical context**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **High Sensitivity** (= lowering the classification threshold) is crucial when the cost of a false negative (i.e., predicting patient as alive when the patient actually dies during his hospital stay) is high. For example, in initial screening tests for serious conditions like cancers, it is preferable to have a high number of true positives, even at the risk of including some false positives. These individuals can then undergo more specific, confirmatory testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **High Specificity** (= increasing the classification threshold) is essential when the cost of a false positive (i.e., predicting that the patient will die when he actually survives his hospital stay) is high. This could involve invasive follow-up procedures, expensive treatments, or significant psychological distress for the patient. For instance, a confirmatory test for a rare genetic disorder or a test to confirm a cancer diagnosis before starting chemotherapy must be highly specific to avoid treating healthy individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this trade-off, it is important to consider alternative evaluation metrics that are not dependent on a single threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, the **Receiver Operating Characteristic (ROC)** curve provides a comprehensive view of a model's performance by plotting the true positive rate (sensitivity) against the false positive rate (1 - specificity) across all possible threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "perfect_clf = RandomForestClassifier()\n",
    "perfect_clf.fit(X, y)\n",
    "good_clf = RandomForestClassifier(n_estimators=100, max_depth=8)\n",
    "good_clf.fit(X, y)\n",
    "average_clf = LogisticRegression()\n",
    "average_clf.fit(X, y)\n",
    "\n",
    "roc_models = {\n",
    "    \"Model 1\": perfect_clf,\n",
    "    \"Model 2\": good_clf,\n",
    "    \"Model 3\": average_clf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(roc_models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The associated metric, the **ROC - Area Under the Curve (AUC)**, is a value that represents the discrimination (i.e., ability to separate patients who survived from patients who died) of the model across all possible probability thresholds. It ranges from 0 to 1, with higher value indicating better discrimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_predictions = clf.predict_proba(X)[:,1]\n",
    "roc_auc_score(y_true=y, y_score=y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now select for each model the hyperparameters values that maximize the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's recall for each model the main hyperparameters that can be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **[Logistic regression (or LASSO)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** :\n",
    "    - Parameter `C` that controls the penalization value imposed on the coefficients of a logistic regression. <br>**A large penalization will force coefficients to 0.** <br>In scikit-learn a smaller value of the hyperparameter `C` indicates higher penalization.\n",
    "\n",
    "- **[K-nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)**:\n",
    "    - Number of neighbors: `n_neighbors`\n",
    "\n",
    "- **[Decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)**:\n",
    "    - Maximum depth a tree can grow: `max_depth`\n",
    "     \n",
    "- **[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**:\n",
    "    - Number of trees in the forest: `n_estimators`\n",
    "    - Number of features considered when creating a split: `max_features`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hyperparameter tuning for LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "C_values = [0.0001, 0.001, 0.01, 0.1, 1, 10] # Define a list of values to test for the penalization\n",
    "\n",
    "lasso_score = {} ## Dictionary to store the AUC for each value of C\n",
    "\n",
    "for k in C_values: ## loop over the different values of C to train a model for each value and store the AUC in the dictionary\n",
    "\n",
    "    clf = LogisticRegression(C=k, penalty='l1', solver='saga', max_iter=500) ## Instantiate a logistic regression model with the current value of C\n",
    "    clf.fit(X, y) ## Train the model\n",
    "    lasso_score[\"C = \" +str(k)] = roc_auc_score(y_true=y, y_score=clf.predict_proba(X)[:,1]) ## Store the AUC in the dictionary with a key that indicates the value of C used for that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(lasso_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "max_depth_values = [1, 3, 5, 10, 15, 20, None] ## Define a list of max_depth values to test for the Decision Tree model\n",
    "\n",
    "decision_tree_score = {} ## Dictionary to store AUC scores for different max_depth values\n",
    "\n",
    "\n",
    "## Loop through each max_depth value\n",
    "for k in max_depth_values:\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=k) ## Initialize a Decision Tree classifier with the current value of max_depth\n",
    "    clf.fit(X, y) ## Train the model on the dataset\n",
    "    decision_tree_score[\"Max_depth = \" + str(k)] = roc_auc_score(y_true=y, y_score=clf.predict_proba(X)[:, 1])  ## Store the AUC in the dictionary with a key that indicates the value of max_depth used for that model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(decision_tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercice**\n",
    "\n",
    "Using the same method, find the best set of hyperparameters for the **random forest** and the **KNN** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hyperparameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "number_of_trees = [...]\n",
    "\n",
    "random_forest_score = {}\n",
    "\n",
    "for k in number_of_trees:\n",
    "    \n",
    "    clf = RandomForestClassifier(...)\n",
    "    clf.fit(X,y)\n",
    "    random_forest_score[\"n_trees = \" +str(k)] = roc_auc_score(y_true=y, y_score=clf.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(random_forest_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hyperparameter tuning for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "number_of_neighbors = [...]\n",
    "\n",
    "knn_score = {}\n",
    "\n",
    "for k in number_of_neighbors:\n",
    "    \n",
    "    clf = KNeighborsClassifier(...)\n",
    "    clf.fit(X, y)\n",
    "    knn_score[\"n_neighbors = \" +str(k)] = roc_auc_score(y_true=y, y_score=clf.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(knn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After finding the optimal value for each model parameter, we can train our models one final time and evaluate their performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LASSO': LogisticRegression(penalty='l1', C=..., solver=\"saga\", max_iter=500),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=...),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=...),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=...)    \n",
    "}\n",
    "for m in models.values():\n",
    "    m.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discrimination (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calibration (training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Model calibration assesses how well the predicted probabilities of a model align with the actual observed outcomes. In a well-calibrated model, if we consider all the instances where the model predicted a certain probability (e.g., 20% chance of mortality), the actual proportion of those instances that experience the event should be close to that probability (i.e., approximately 20% of them should have died)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**How to interpret a calibration plot:**\n",
    "-   **Axes:** The x-axis represents the predicted probabilities, and the y-axis shows the actual observed frequency of the positive class for those probabilities.\n",
    "-   **Perfectly Calibrated (Diagonal Line):** The dashed diagonal line represents perfect calibration, where the predicted probabilities exactly match the observed frequencies.\n",
    "-   **Model's Curve:** The solid lines show the calibration of each model.\n",
    "    -   If a model's curve is **below** the diagonal, it is **over-predicting** the risk (the predicted probabilities are higher than the actual outcomes).\n",
    "    -   If a model's curve is **above** the diagonal, it is **under-predicting** the risk (the predicted probabilities are lower than the actual outcomes).\n",
    "\n",
    "The closer a model's curve is to the diagonal, the better its calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(models, X, y, curve_type='calibration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our models perform well on the training data, but ultimately, the goal is to ensure strong predictive performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let’s put that to the test by loading a new set of observations (`icu_testing.csv`) and evaluating our models’ performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/AL0UNE/courses/refs/heads/main/icu_testing.csv'\n",
    "\n",
    "df_test = pd.read_csv(url_data, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before evaluating the performance of our models on this new dataset, we must apply the same transformations that was done on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_test = df_test[keep]\n",
    "\n",
    "df_test = df_test.fillna({\"height\": mean_height, \"first_icu_stay\": most_frequent_first_icu_stay})\n",
    "\n",
    "df_test['gender'] = df_test['gender'].replace({'M': 1, 'F': 0})\n",
    "df_test['first_icu_stay'] = df_test['first_icu_stay'].replace({True: 1, False: 0})\n",
    "df_test['CURR_CAREUNIT_transfers'] = encoder.transform(df_test['CURR_CAREUNIT_transfers'])\n",
    "\n",
    "df_test['BMI'] = df_test['weight']/((df_test['height']/100)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test[X.columns]\n",
    "y_test = df_test['hospital_mortality']\n",
    "X_test = (X_test-train_mean)/train_std ## Standardize using statistics from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we can assess the performance of our models on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(models, X, y, X_test, y_test, figsize = (8, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(models, X, y, X_test, y_test, curve_type='calibration', figsize = (8, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When evaluating on the test set, we observe a decline in performance for most models, both in terms of discrimination and calibration. <br>\n",
    "The LASSO model is the only one that maintains its predictive quality across both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, we selected hyperparameters to maximize performance, so what went wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that **we tuned the hyperparameters and evaluated models' performance on the same (training) dataset.** This approach leads to an overly optimistic assessment of the model's capabilities, as the model is optimized for the specific data it has already seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This phenomenon is known as **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "To understand what **overfitting** and **underfitting** are, let's go back to the toy example from last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/AL0UNE/courses/blob/main/figures/synthetic_data_for_under_over_fitting.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we will train three different decision tree classifiers, each with a different value of **max depth** (**1, 4 and 10**), and observe their decision boundaries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/AL0UNE/courses/blob/main/figures/Underfitting_Overfitting.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From left to right, we show classifiers of increasing complexity, with more complex decision boundaries (i.e. the decision boudary is more and more wiggly). <br> In fact, the classifier on the right perfectly classifies all training data points, also indicated by the perfect ROC-AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But is choosing the model with `max_depth = 10` the best choice? The model with `max_depth = 4` (middle figure) appears to be more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, how can we systematically choose the best hyperparameter value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A question of trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let’s plot the AUC score for both the training and testing sets as we increase the `max_depth` for a classification tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/AL0UNE/courses/blob/main/figures/Model_complexity.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When a model is too simple relative to the information in the training data, it is said to have *high bias*.<br>\n",
    "The consequence is ***underfitting***.\n",
    "- The decision tree model with `max_depth=1` shown on the left is an example.\n",
    "- It has a poor AUC on both the training and the validation set: the model did not learn any useful pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On the other hand, when a model is too complex given the training data, it is said to have *high variance*. <br>\n",
    "The consequence is ***overfitting***.\n",
    "\n",
    "- The decision tree model with `max_depth=20` shown on the right is an example.\n",
    "- It has a perfect AUC on the training set but a poor AUC on the testing set: the model has learned not just the underlying patterns, but also the noise present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Methods for estimating model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/AL0UNE/courses/blob/main/figures/initial_data.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this approach, we simply split the initial dataset into training and testing sets (e.g., 70% for training and 30% for testing). <br>\n",
    "Although using the test set to evaluate model performance on new data seems reasonable, it can lead to overfitting on the test set itself. This is undesirable because the test set should only be used for assessing the model's final performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train-Validation-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/AL0UNE/courses/blob/main/figures/train_test_split.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another approach consists in simply further splitting the training set to create a validation set, which can be used for hyperparameter tuning and it remains independent from the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the function to split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Defining a list of max_depth values to test for the Decision Tree model\n",
    "max_depth = [1, 3, 5, 10, 15, 20, None] \n",
    "\n",
    "# Dictionary to store AUC scores for different max_depth values\n",
    "sample_split_score = {}\n",
    "\n",
    "# Splitting the dataset into training (70%) and validation (30%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through each max_depth value to train and evaluate the model\n",
    "for k in max_depth:\n",
    "\n",
    "    clf = DecisionTreeClassifier(max_depth=k) # Initializing a Decision Tree classifier with the specified max_depth\n",
    "    clf.fit(X_train, y_train) # Training the model on the training data\n",
    "    sample_split_score[\"max_depth = \" + str(k)] = roc_auc_score(y_true=y_val, y_score=clf.predict_proba(X_val)[:, 1]) # Evaluating the model on the validation set and storing the AUC score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(sample_split_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the train-validation-testing splitting strategy, we observe that a **max_depth** of **5** is preferable, which is different from the value we obtained with the naive method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "training_score = {}\n",
    "for k in max_depth_values:\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=k)\n",
    "    clf.fit(X_train,y_train)\n",
    "    training_score[\"max_depth = \" + str(k)] = roc_auc_score(y_true=y_train, y_score=clf.predict_proba(X_train)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(training_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/AL0UNE/courses/blob/main/figures/crosval.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**K-Fold cross-validation** is an extension of the train/validation splitting method, where instead of creating a single validation set, the process is repeated multiple times (K times).\n",
    "\n",
    "The idea behind cross-validation is to divide the training data into several *folds*. In each iteration, one fold serves as the validation set, while the remaining folds are used for training. This process is repeated until every fold has been used as the validation set.\n",
    "\n",
    "\n",
    "The final performance is averaged across all K iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The good news is that you can use the `cross_val_score` function from the `scikit-learn` library to handle most of the work for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the cross-validation function from scikit-learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Defining a list of max_depth values to test for the Decision Tree model\n",
    "max_depth = [1, 3, 5, 10, 15, 20, None] \n",
    "\n",
    "# Dictionary to store the AUC scores from cross-validation\n",
    "cross_val_decision_tree = {}\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through each max_depth value\n",
    "for k in max_depth:\n",
    "    # Initializing a Decision Tree classifier with the specified max_depth\n",
    "    clf = DecisionTreeClassifier(max_depth=k)\n",
    "    \n",
    "    # Performing cross-validation with 5 folds and calculating the ROC-AUC score\n",
    "    cross_val_decision_tree[\"max_depth = \" + str(k)] = cross_val_score(estimator=clf, X=X, y=y, cv=n_folds, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can then average over all folds and take the parameter that maximize the average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_decision_tree).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Leave-one-out: a special case of k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/AL0UNE/courses/blob/main/figures/loo.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Leave-One-Out Cross-Validation (LOO-CV)** is a special case of cross-validation where K equals to the number of data points. In each iteration, the model is trained on all but one data point, and that single data point is used as the test set. This method can be computationally expensive, particularly for large datasets, as it requires training the model once for each data point.\n",
    "\n",
    "LOO-CV is especially useful when the dataset is small, as it allows for maximum utilization of the available data for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many other ways to partition the data. See the following scikit-learn page for a [non-exhaustive list of methods](https://scikit-learn.org/stable/modules/cross_validation.html) for data-partitioning and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now have different methods to tune our models effectively without overfitting to the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b>Exercise:</b>\n",
    "\n",
    "- Using the cross-validation method, tune the hyperparameters for the LASSO (`C`), the KNN (`n_neighbors`), and the Random Forest models (`n_estimators`). <br>You can also experiment with different numbers of folds for cross-validation and observe how it impacts the choice of parameters.\n",
    "- Compare with the values obtained with the naive method using the training data for training and optimization. How did the values change?  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "C_values = [...] # Define a list of values to test for the penalization\n",
    "\n",
    "cross_val_lasso = {} ## Dictionary to store the AUC for each value of C\n",
    "\n",
    "for k in C_values: \n",
    "\n",
    "    clf = LogisticRegression(C=..., penalty='l1', solver='saga', max_iter=500) \n",
    "    \n",
    "    cross_val_lasso[\"C = \" +str(k)] = cross_val_score(clf, X, y, cv=n_folds, scoring='roc_auc') ## Perform cross-validation and store the AUC in the dictionary with a key that indicates the value of C used for that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_lasso).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "n_neighbors = [...] # Define a list of values to test for the number of neighbors in KNN\n",
    "\n",
    "cross_val_knn = {} ## Dictionary to store the AUC for each value of neighbors\n",
    "\n",
    "for k in n_neighbors: \n",
    "\n",
    "    clf = ...\n",
    "    \n",
    "    cross_val_knn[\"n_neighbors = \" +str(k)] = cross_val_score(clf, X, y, cv=n_folds, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_knn).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "n_trees = [...] # Define a list of values to test for the number of trees in the Random Forest model \n",
    "\n",
    "cross_val_random_forest = {} ## Dictionary to store the AUC for each number of trees \n",
    "\n",
    "for k in n_trees: \n",
    "\n",
    "    clf = ...\n",
    "    \n",
    "    cross_val_random_forest[\"n_estimators = \" +str(k)] = cross_val_score(clf, X, y, cv=n_folds, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_random_forest).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gridsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most machine learning models have multiple hyperparameters that can be tuned to optimize performance. For instance, for a **Random Forest model**, these include:\n",
    "- The number of trees in the forest (`n_estimators`)\n",
    "- The number of features considered for each split (`max_features`)\n",
    "- The maximum depth of each tree (`max_depth`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With multiple parameters to tune, the number of unique combinations can quickly become large which can be tedious to keep a track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fortunately, scikit-learn provides a powerful function called `GridSearchCV` to automate this process. `GridSearchCV` performs an exhaustive search over a specified hyperparameter grid. For each combination of hyperparameters, it evaluates the model's performance using cross-validation and selects the set of values that yields the best score on a chosen metric (e.g., ROC-AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Importing GridSearchCV from scikit-learn for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initializing the RandomForestClassifier model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Defining a grid of hyperparameters to search over for the Random Forest model\n",
    "random_forest_grid = { \n",
    "    'n_estimators': [5, 20, 100],  # Number of trees in the forest\n",
    "    'max_depth': [1, 3],        # Maximum depth of each tree\n",
    "    'max_features': [1, 3, 5],     # Number of features to consider when splitting a node\n",
    "}\n",
    "\n",
    "\n",
    "# Defining the number of folds for cross-validation during the gridsearch\n",
    "n_folds = 5\n",
    "\n",
    "# Setting up the GridSearchCV to perform hyperparameter tuning\n",
    "gs_result = GridSearchCV(\n",
    "    estimator=clf,               # The classifier to evaluate\n",
    "    param_grid=random_forest_grid, # The grid of hyperparameters to search over\n",
    "    scoring='roc_auc',            # The scoring metric (ROC-AUC)\n",
    "    cv=n_folds,                   # Number of folds for cross-validation\n",
    "    n_jobs=-1,                    # Using all CPU cores for parallel processing\n",
    "    verbose=1                      # Verbosity level for progress display\n",
    ")\n",
    "\n",
    "# Fitting the GridSearchCV to the data and finding the best hyperparameters\n",
    "gs_result.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can you explain the No. of candidates and the total No. of fits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search_results = pd.DataFrame(gs_result.cv_results_)\n",
    "grid_search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each combination of parameters, we get a as many AUCs as there are folds. We can then choose the combination of parameters that has the highest mean AUC across all k-folds (`mean_test_score` or `rank_test_score`column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search_results.sort_values('rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Based on our predefined grid, the best set of hyperparameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_search_results.sort_values('rank_test_score').iloc[0]['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise:</b>\n",
    "\n",
    "- Use the GridSearchCV method to find the optimal hyperparameters for the Random Forest algorithm using the hyperparameter grid by changing values or adding additional [hyperparameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "- (**Optional**): Apply the same approach to optimize multiple hyperparameters for a [classification tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and for the [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classifier (take a look at the documentation to see a list of the different hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the optimal hyperparameters are determined for each model, we train the final models using the full training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CclavAt23vYz",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LASSO': LogisticRegression(penalty='l1', C=..., solver=\"saga\", max_iter=500),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=...),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=..., max_depth=..., max_features=...),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=...),\n",
    "}\n",
    "\n",
    "for m in models.values():\n",
    "    m.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we have trained our different models, we can evaluate their performance on the test set we have set aside.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(models, X, y, X_test, y_test, figsize = (8, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(models, X, y, X_test, y_test, curve_type='calibration', figsize = (8, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After training a model, we can analyze which the importance of features on average for each model. This helps assess whether the model has focused on clinically meaningful patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(models, X, figsize=(15, 6)):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize, sharey=True)    \n",
    "    ax[0].tick_params(axis='y', labelsize=12)\n",
    "    for i, (name,model) in enumerate(models.items()):\n",
    "        if hasattr(model, 'coef_'):\n",
    "            ax[i].barh(X.columns, np.abs(model.coef_[0]))\n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            ax[i].barh(X.columns, model.feature_importances_)            \n",
    "        else:\n",
    "            continue\n",
    "        ax[i].set_title(name, fontsize=15)\n",
    "        ax[i].set_xlabel('Feature importance', fontsize=12)\n",
    "        ax[i].set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(models, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One key advantage of (shallow) decision trees is their interpretability. Scikit-learn provides a function to visualize and save the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,20))\n",
    "plot_tree(models['Decision Tree'], feature_names=X.columns, class_names=['Survived', 'Died'], fontsize=6, proportion=False, filled=True, rounded=True)\n",
    "plt.savefig('my_decision_tree', dpi=100)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "colab": {
   "name": "TP_DU_IA_SANTE_jupyter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "alan2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
